{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expt_name=\"seismicTL_circular2\"\n",
    "#fname=\"/home/pawan/Dropbox/SymAE/\"+expt_name#\n",
    "fnametest=\"/home/pawan/Dropbox/SymAE/test2\"+expt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=\"./train_data\"\n",
    "fnametest=\"./test0data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run ../core.py\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=h5py.File(fname+\".h5\", 'r')\n",
    "dattest=h5py.File(fnametest+\".h5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Xo_train\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dat)\n\u001b[0;32m----> 2\u001b[0m nsamp, ntau, n1, n2, nfilt \u001b[38;5;241m=\u001b[39m Xo_train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "Xo_train=np.array(dat)\n",
    "nsamp, ntau, n1, n2, nfilt = Xo_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Xo_train\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-0.005407926971061715\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m nsamp, ntau, n1, n2, nfilt \u001b[38;5;241m=\u001b[39m Xo_train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "Xo_train=np.array(dat['-0.005407926971061715'])\n",
    "nsamp, ntau, n1, n2, nfilt = Xo_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Xo_train\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-0.005407926971061715\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m nsamp, ntau, n1, n2, nfilt \u001b[38;5;241m=\u001b[39m Xo_train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "Xo_train=np.array(dat['-0.005407926971061715']['data'])\n",
    "nsamp, ntau, n1, n2, nfilt = Xo_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Xo_test=np.repeat(Xo_test,4,axis=0)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Xo_train=np.expand_dims(Xo_train,[3])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Xo_test=np.expand_dims(Xo_test,[4])\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nsamp, ntau, n1, n2, nfilt\u001b[38;5;241m=\u001b[39mXo_train\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(fname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     labels_train \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "# Xo_test=np.repeat(Xo_test,4,axis=0)\n",
    "# Xo_train=np.expand_dims(Xo_train,[3])\n",
    "# Xo_test=np.expand_dims(Xo_test,[4])\n",
    "nsamp, ntau, n1, n2, nfilt=Xo_train.shape\n",
    "with open(str(fname+'.json')) as f:\n",
    "    labels_train = json.load(f)\n",
    "labels_train=[labels_train[i] for i in range(0,nsamp)]\n",
    "#with open(str(fnametest+'.json')) as f:\n",
    "#    labels_test = json.load(f)\n",
    "#labels_test=[labels_test[str(i)] for i in range(1,3)]\n",
    "\n",
    "print(\"shape of Xo_train\",np.shape(Xo_train))\n",
    "print(\"shape of Xo_test\",np.shape(Xo_test))\n",
    "print(\"number of samples Xo_train\\t\", nsamp)\n",
    "print(\"number of instances in each datapoint Xo_train\\t\", ntau)\n",
    "print(\"number of samples in each channel\\t\", (n1,n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Xo_train, labels_train, Xo_test, labels_test=read_datapoints(fname, fnametest)\n",
    "# nsamp, ntau,n1,n2,nfilt=Xo_train.shape\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils import shuffle\n",
    "# Xo_train, Xo_test=train_test_split(Xo, test_size=0.1, shuffle=True)\n",
    "# Xo_train, labels_train=shuffle(Xo_train, labels_train, n_samples=9000)\n",
    "# Xo_train, Xo_test, labels_train, labels_test=train_test_split(Xo_train, labels_train, test_size=0.1, shuffle=True)\n",
    "# nsamp, ntau, n1,n2,nfilt =Xo_train.shape\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 13:50:11.413663: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "# %load ../symae_core.py\n",
    "\n",
    "#%% Load packages\n",
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfkltd= tf.keras.layers.TimeDistributed\n",
    "\n",
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "class NuisanceEncoder(tf.keras.Model):\n",
    "  def __init__(self, kernel_sizes, filters, rdown=[2,2,1,1], tdown=[1,1,2,2], fstep=[2,4,8], latent_dim=512):\n",
    "    super(NuisanceEncoder, self).__init__(name='')\n",
    "    k1,k2=kernel_sizes\n",
    "\n",
    "    # self.c1=tfkltd(tfkl.Conv2D(filters,(k1,k2),activation='elu'))\n",
    "    self.c1=tfkltd(tfkl.Conv2D(filters,(k1,k2),strides=(rdown[0],tdown[0]),activation='elu'))\n",
    "    # self.mp1=tfkltd(tfkl.MaxPool2D(pool_size=(rdown[0],tdown[0])))\n",
    "    # self.c3=tfkltd(tfkl.Conv2D(filters//fstep[0],(k1,k2),activation='elu'))\n",
    "    self.c4=tfkltd(tfkl.Conv2D(filters//fstep[0],(k1,k2),strides=(rdown[1],tdown[1]),activation='elu'))\n",
    "    # self.mp2=tfkltd(tfkl.MaxPool2D(pool_size=(rdown[1],tdown[1])))\n",
    "    self.c5=tfkltd(tfkl.Conv2D(filters//fstep[1],(k1,k2),strides=(rdown[2],tdown[2]),activation='elu'))\n",
    "    # self.c6=tfkltd(tfkl.Conv2D(filters//fstep[1],(k1,k2),activation='elu'))\n",
    "    # self.mp3=tfkltd(tfkl.MaxPool2D(pool_size=(rdown[2],tdown[2])))\n",
    "    # self.c7=tfkltd(tfkl.Conv2D(filters//fstep[2],(k1,k2),activation='elu'))\n",
    "    self.c8=tfkltd(tfkl.Conv2D(filters//fstep[2],(k1,k2),strides=(rdown[3],tdown[3]),activation='elu'))\n",
    "    self.bn=tfkltd(tfkl.BatchNormalization())\n",
    "    # self.mp4=tfkltd(tfkl.MaxPool2D(pool_size=(rdown[3],tdown[3])))\n",
    "    self.f=tfkltd(tfkl.Flatten())\n",
    "    self.d=tfkltd(tfkl.Dense(latent_dim))\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    # n, ntau, nr, nt, nc = input_tensor.get_shape()\n",
    "    x=self.c1(input_tensor)\n",
    "    # x=self.c2(x)\n",
    "    # x=self.mp1(x)\n",
    "    # x=self.c3(x)\n",
    "    x=self.c4(x)\n",
    "    # x=self.mp2(x)\n",
    "    x=self.c5(x)\n",
    "    # x=self.c6(x)\n",
    "    # x=self.mp3(x)\n",
    "    # x=self.c7(x)\n",
    "    x=self.c8(x)\n",
    "    x=self.bn(x, training=training)\n",
    "    # x=self.mp4(x)\n",
    "    x=self.f(x)\n",
    "    out=self.d(x)\n",
    "    return out\n",
    "  def model(self, x):\n",
    "    return tfk.Model(inputs=x, outputs=self.call(x))\n",
    "\n",
    "\n",
    "class SymmetricEncoder(tf.keras.Model):\n",
    "  def __init__(self, kernel_sizes, filters, rdown=[2,2,2,1], tdown=[2,4,4,4],latent_dim=8, fstep=[2,4,8]):\n",
    "    super(SymmetricEncoder, self).__init__(name='')\n",
    "    k1,k2=kernel_sizes\n",
    "\n",
    "    self.c11=tfkltd(tfkl.Conv2D(filters,(k1,k2),activation='elu'))\n",
    "    self.c12=tfkltd(tfkl.Conv2D(filters//fstep[0],(k1,k2),strides=(rdown[0],tdown[0]),activation='elu'))\n",
    "    # self.mp11=tfkltd(tfkl.MaxPool2D(pool_size=(rdown[0],tdown[0])))\n",
    "    self.c13=tfkltd(tfkl.Conv2D(filters//fstep[1],(k1,k2),activation='elu'))\n",
    "    self.c14=tfkltd(tfkl.Conv2D(filters//fstep[2],(k1,k2),strides=(rdown[1],tdown[1]),activation='elu'))\n",
    "    # self.mp12=tfkltd(tfkl.MaxPool2D(pool_size=(rdown[1],tdown[1])))\n",
    "\n",
    "\n",
    "    self.c21=tfkl.Conv2D(filters,(k1,k2),strides=(rdown[2],tdown[2]),activation='elu')\n",
    "    # self.c22=tfkl.Conv2D(filters//fstep[0],(k1,k2),padding='same',activation='elu')\n",
    "    # self.mp21=tfkl.MaxPool2D(pool_size=(rdown[2],tdown[2]))\n",
    "    # self.c23=tfkl.Conv2D(filters//fstep[1],(k1,k2),padding='same',activation='elu')\n",
    "    self.c24=tfkl.Conv2D(filters//fstep[2],(k1,k2),strides=(rdown[3],tdown[3]),activation='elu')\n",
    "    self.bn=tfkl.BatchNormalization()\n",
    "    # self.mp22=tfkl.MaxPool2D(pool_size=(rdown[3],tdown[3]))\n",
    "    self.f=tfkl.Flatten()\n",
    "    self.d=tfkl.Dense(latent_dim)\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    # n, ntau, nr, nt, nc = input_tensor.get_shape()\n",
    "    x=self.c11(input_tensor)\n",
    "    x=self.c12(x)\n",
    "    # x=self.mp11(x)\n",
    "    x=self.c13(x)\n",
    "    x=self.c14(x)\n",
    "    # x=self.mp12(x)\n",
    "    x=tf.math.reduce_mean(x,axis=1)\n",
    "    x=self.c21(x)\n",
    "    # x=self.c22(x)\n",
    "    # x=self.mp21(x)\n",
    "    # x=self.c23(x)\n",
    "    x=self.c24(x)\n",
    "    x=self.bn(x, training=training)\n",
    "    # x=self.mp22(x)\n",
    "    x=self.f(x)\n",
    "    out=self.d(x)\n",
    "    return out\n",
    "  def model(self, x):\n",
    "    return tfk.Model(inputs=x, outputs=self.call(x))\n",
    "\n",
    "\n",
    "class DistributeZsym(tf.keras.Model):\n",
    "  def __init__(self, ntau, nz0, nzi):\n",
    "    super(DistributeZsym, self).__init__(name='')\n",
    "\n",
    "    self.nz0=nz0\n",
    "    self.nzi=nzi\n",
    "    self.ntau=ntau\n",
    "    self.ri=tfkl.Reshape(target_shape=(ntau,nzi))\n",
    "    self.repeat=tfkl.RepeatVector(ntau)\n",
    "#     Xhatr=tfkl.Reshape(target_shape=(ntau,latent_dimr))(Xhatr)\n",
    "\n",
    "  def call(self, z, training=False):\n",
    "\n",
    "    z0,zi=tf.split(z,[self.nz0, self.ntau*self.nzi],axis=1)\n",
    "    zi=self.ri(zi)\n",
    "    z0=self.repeat(z0)\n",
    "    out=tfkl.concatenate([z0, zi],axis=2)\n",
    "    return out\n",
    "  def model(self, x):\n",
    "    return tfk.Model(inputs=x, outputs=self.call(x))\n",
    "\n",
    "\n",
    "class LatentCat(tf.keras.Model):\n",
    "  def __init__(self, alpha=1.0):\n",
    "    super(LatentCat, self).__init__(name='')\n",
    "\n",
    "    self.drop=tfkl.GaussianDropout(alpha)\n",
    "    # self.drop=tfkl.Dropout(alpha)\n",
    "\n",
    "  def call(self, zsym, znuisance,training=False):\n",
    "    znuisance=self.drop(znuisance,training=training)\n",
    "    z=tfkl.concatenate([zsym, znuisance])\n",
    "    return z\n",
    "\n",
    "\n",
    "class Mixer(tf.keras.Model):\n",
    "  def __init__(self, kernel_sizes, filters, upfacts, nt, nr, fstep=[8,4,2]):\n",
    "    super(Mixer, self).__init__(name='')\n",
    "    k1,k2=kernel_sizes\n",
    "    rup,tup=upfacts\n",
    "\n",
    "    filt_in=64\n",
    "\n",
    "    self.d1=tfkltd(tfkl.Dense(units=((nr//rup)*(nt//tup)*filt_in),activation='elu'))\n",
    "    self.r2=tfkltd(tfkl.Reshape(target_shape=((nr//rup),(nt//tup),filt_in)))\n",
    "    self.c1=tfkltd(tfkl.Conv2D(filters//fstep[0],(k1,k2),padding='same',activation='elu'))\n",
    "    self.us1=tfkltd(tfkl.UpSampling2D(size=(rup,tup)))\n",
    "    # self.c2=tfkltd(tfkl.Conv2D(filters//fstep[1],(k1,k2),padding='same',activation='elu'))\n",
    "    self.c3=tfkltd(tfkl.Conv2D(filters//fstep[1],(k1,k2),padding='same',activation='elu'))\n",
    "    self.bn=tfkltd(tfkl.BatchNormalization())\n",
    "    self.c4=tfkltd(tfkl.Conv2D(filters//fstep[2],(k1,k2),padding='same',activation='elu'))\n",
    "    # self.c5=tfkltd(tfkl.Conv2D(filters//fstep[2],(k1,k2),padding='same',activation='elu'))\n",
    "    self.c6=tfkltd(tfkl.Conv2D(1,(k1,k2),padding='same'))\n",
    "\n",
    "  def call(self, z, training=False):\n",
    "    x=self.d1(z)\n",
    "    x=self.r2(x)\n",
    "    x=self.us1(x)\n",
    "    x=self.c1(x)\n",
    "    # x=self.c2(x)\n",
    "    x=self.c3(x)\n",
    "    x=self.bn(x, training=training)\n",
    "    x=self.c4(x)\n",
    "    # x=self.c5(x)\n",
    "    out=self.c6(x)\n",
    "    return out\n",
    "  def model(self, x):\n",
    "    return tfk.Model(inputs=x, outputs=self.call(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 13:50:28.526167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 13:50:29.963446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2022-09-23 13:50:29.964408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13883 MB memory:  -> device: 1, name: NVIDIA RTX A4000, pci bus id: 0000:1b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ntau' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# strategy = tf.distribute.MirroredStrategy()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\",\"/gpu:2\",\"/gpu:3\",\"/gpu:4\",\"/gpu:5\",\"/gpu:6\",\"gpu:7\"])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# strategy = tf.distribute.OneDeviceStrategy(\"/gpu:0\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# strategy = tf.distribute.experimental.CentralStorageStrategy() \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#%% model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m---> 12\u001b[0m     encoder_input\u001b[38;5;241m=\u001b[39mtfk\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[43mntau\u001b[49m,n1,n2,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     downsampler=Downsampler([2,25],8,2)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     downsampler=DownsamplerDense(200,nt)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     xd=downsampler(encoder_input)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     downsampler.summary(line_length=120)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     tf.keras.utils.plot_model( downsampler, to_file='encoderdown.png', show_shapes=True, 4\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ntau' is not defined"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\",\"/gpu:2\",\"/gpu:3\",\"/gpu:4\",\"/gpu:5\",\"/gpu:6\",\"gpu:7\"])\n",
    "# strategy = tf.distribute.OneDeviceStrategy(\"/gpu:0\")\n",
    "# strategy = tf.distribute.get_strategy()\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"gpu:7\"])\n",
    "# strategy = tf.distribute.experimental.CentralStorageStrategy() \n",
    "#%% model\n",
    "with strategy.scope():\n",
    "    \n",
    "    \n",
    "    encoder_input=tfk.Input(shape=(ntau,n1,n2,1), dtype='float32', name='encoder_input')\n",
    "    \n",
    "#     downsampler=Downsampler([2,25],8,2)\n",
    "#     downsampler=DownsamplerDense(200,nt)\n",
    "#     xd=downsampler(encoder_input)\n",
    "\n",
    "#     downsampler.summary(line_length=120)\n",
    "#     tf.keras.utils.plot_model( downsampler, to_file='encoderdown.png', show_shapes=True, 4\n",
    "    kernel_size=(5,5)\n",
    "    nfilt=64\n",
    "    \n",
    "    nz0=10\n",
    "    symencoder1=SymmetricEncoder(kernel_size,nfilt,[2,2,2,2],[2,2,2,2],nz0, [1,1,1])\n",
    "    xz0=symencoder1(encoder_input)\n",
    "    \n",
    "    z0=tfkl.Flatten()(xz0)\n",
    "    \n",
    "    nz0=z0.get_shape()[1]\n",
    "    \n",
    "    report(symencoder1.model(encoder_input),\"symencoder\")\n",
    "    symencoder=tf.keras.Model(encoder_input, z0, name=\"symencoder\")\n",
    "    \n",
    "    \n",
    "    nzi=200\n",
    "    nencoder1=NuisanceEncoder(kernel_size,nfilt,[2,2,2,2],[2,2,2,2],[1,1,1],nzi)\n",
    "    \n",
    "    xzi=nencoder1(encoder_input)\n",
    "\n",
    "    zi=tfkl.Flatten()(xzi)\n",
    "\n",
    "    report(nencoder1.model(encoder_input),\"nencoder\")\n",
    "    nencoder=tf.keras.Model(encoder_input, zi, name=\"nencoder\")\n",
    "    \n",
    "    decoder_input = tf.keras.Input(shape=(nzi*ntau+nz0), name='latentcode')\n",
    "    \n",
    "    distzsym=DistributeZsym(ntau, nz0, nzi)\n",
    "    xdhat=distzsym(decoder_input)\n",
    "    \n",
    "    report(distzsym.model(decoder_input),\"distzsym\")\n",
    "\n",
    "    mixer=Mixer(kernel_size,nfilt,(10,10),n2,n1,[1,1,1])\n",
    "    xhat=mixer(xdhat)\n",
    "\n",
    "#     upsampler=Upsampler([2,25],8,2)\n",
    "#     upsampler=UpsamplerDense(nt,200)\n",
    "#     xhat=upsampler(xdhat)\n",
    "    \n",
    "    znuisance=nencoder(encoder_input)\n",
    "    zsym=symencoder(encoder_input)\n",
    "    \n",
    "    \n",
    "    report(mixer.model(tfk.Input(shape=(ntau,nzi+nz0))),\"mixer\")\n",
    "    \n",
    "#     report(upsampler,\"upsampler\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "    latentcat=LatentCat(0.4)\n",
    "    \n",
    "    encoder=tf.keras.Model(encoder_input, latentcat(zsym,znuisance), name=\"encoder\")\n",
    "\n",
    "    decoder=tf.keras.Model(decoder_input, xhat, name=\"decoder\")\n",
    "    report(decoder, \"decoder\")\n",
    "    \n",
    "    model=tf.keras.Model(encoder_input, decoder(latentcat(zsym,znuisance)) , name='autoencoder')\n",
    "    report(model, \"autoencoder\")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "ds_train=tf.data.Dataset.from_tensor_slices((Xo_train, Xo_train))\n",
    "# ds_train = tf.data.TFRecordDataset(filenames=[\"Xo.tfrecords\"])\n",
    "#   print(ds_train)\n",
    "ds_train = ds_train.shuffle(nsamp,reshuffle_each_iteration=True) \n",
    "ds_train=ds_train.batch(batch_size)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test=tf.data.Dataset.from_tensor_slices((Xo_test, Xo_test))\n",
    "ds_test=ds_test.batch(batch_size)\n",
    "# ds_test = ds_test.cache()\n",
    "# ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.noop_elimination = True\n",
    "options.experimental_optimization.map_vectorization.enabled = True\n",
    "# options.experimental_threading.max_intra_op_parallelism = 1 # reduces CPU usage?\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "ds_train = ds_train.with_options(options)\n",
    "# ds_test = ds_test.with_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=base_dir + expt_name + '_TFmodel_save', save_weights_only=True, verbose=1)\n",
    "#model.fit(Xr,Xr,epochs=10, batch_size=batch_size,  callbacks=[cp_callback, generate_and_save_images(), history], validation_split=0.1, shuffle=True)\n",
    "#model.fit(Xr,Xr,epochs=10, batch_size=batch_size,  callbacks=[generate_and_save_images(), history], validation_split=0.1, shuffle=True)\n",
    "# model.fit(Xo,Xo,epochs=100, batch_size=batch_size, callbacks=[tbCallBack], shuffle=True, validation_split=0.05)\n",
    "history=model.fit(ds_train,epochs=50,  shuffle=True, validation_data=ds_test,)\n",
    "# model.fit(ds_train,epochs=40 )\n",
    "# model.fit(Xo,Xo,epochs=1,  batch_size=4)#, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xo_train.dtype, Xo_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seismic(Xo_test, labels_train,symencoder, nencoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Previous Dropout \"+str(model.layers[2].layers[-1].rate))\n",
    "model.save_weights(\"temp_weights\")\n",
    "new_dropout=0.5\n",
    "model.layers[2].layers[-1].rate=new_dropout\n",
    "model = tfk.models.clone_model(model)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.load_weights(\"temp_weights\")\n",
    "print(\"New Dropout \"+str(model.layers[2].layers[-2].rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelindices={'base0': [], 'moni1' : [], 'moni2' : []}\n",
    "for (i,label) in enumerate(labels):\n",
    "    labelindices[label[0][0]].append(i)\n",
    "\n",
    "\n",
    "# In[281]:\n",
    "\n",
    "\n",
    "labels[labelindices['moni2']][0][0]\n",
    "\n",
    "\n",
    "# # Visualize Latent Space\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "# visualizing latent space\n",
    "\n",
    "\"\"\"\n",
    "fig=plt.figure(dpi=100, figsize=(15,8))\n",
    "for label in ['100', '200', '300', '400', '500']:\n",
    "    i = labelindices[label][0:1]\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        latentsr=encoderr(Xo[i])\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "with tf.device('/gpu'):\n",
    "    test_large_indices = [np.random.randint(1, nsamp) for i in range(0,64)]\n",
    "    latentsT=nencoder.predict(Xo_train                           [test_large_indices])\n",
    "    latentsr=symencoder.predict(Xo_train[test_large_indices])\n",
    "    #print(np.shape(latentsr))\n",
    "    fig, axs = plt.subplots(2, 2, dpi=100, figsize=(15,8))\n",
    "    a=axs[0,0].imshow(latentsr, aspect=1)\n",
    "    #for l in latentsr:\n",
    "    #    a=axs[0,0].plot(l)\n",
    "    axs[0,0].set_title('G')\n",
    "    plt.colorbar(a, ax=axs[0,0]) \n",
    "\n",
    "    #a=axs[1,0].imshow(tf.exp(latentsr[1]), aspect=1)\n",
    "    #axs[1,0].set_title('sigma encoderr output for samples')\n",
    "    #plt.colorbar(a, ax=axs[1,0]) \n",
    "\n",
    "    b=axs[0,1].imshow(latentsT, aspect=1)\n",
    "    axs[0,1].set_title('Z')\n",
    "    plt.colorbar(b, ax=axs[0,1]) \n",
    "    \n",
    "plt.gcf()\n",
    "#b=axs[1,1].imshow(tf.exp(latentsT[1]), aspect=1)\n",
    "#axs[1,1].set_title('sigma encoderT output for samples')\n",
    "#plt.colorbar(b, ax=axs[1,1]) \n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "plt.imshow(latentsT)\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "with tf.device('/gpu'):\n",
    "    latentsr={'base0': [], 'moni1':[],'moni2':[]}\n",
    "    for label in ['base0', 'moni1', 'moni2']:\n",
    "        i = labelindices[label][1:20]\n",
    "        with tf.device('/gpu:0'):\n",
    "            latentsr[label]=symencoder.predict(Xo_train[i])\n",
    "\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,5,dpi=500, figsize=(3,2))\n",
    "markers=['r', 'b', 'c', 'g', 'm']\n",
    "subs=['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "for (i,label) in enumerate(['base0', 'moni1', 'moni2', 'base0', 'base0']):\n",
    "    lmean=np.mean(latentsr[label], axis=0)\n",
    "    #ax[i].plot(lmean, color='k', linewidth=0.1)\n",
    "    ax[1,i].imshow(latentsr[label].T)\n",
    "    # Hide grid lines\n",
    "    ax[1,i].grid(False)\n",
    "#     Hide axes ticks\n",
    "    ax[1,i].set_xticks([])\n",
    "    ax[1,i].set_yticks([])\n",
    "    ax[1,i].set_xlabel('$\\tau$')\n",
    "    ax[1,i].set_ylabel('$Z_g$')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax[0,i].imshow(vp[label].T, cmap='RdBu')\n",
    "    ax[0,i].axis('off')\n",
    "    ax[0,i].set_xlabel('$x$')\n",
    "    ax[0,i].set_ylabel('$z$')\n",
    "    ax[0,i].set_title(subs[i]+'$\\epsilon=$'+str(i))\n",
    "\n",
    "\n",
    "    #for l in latentsr[label]:\n",
    "     #   ax[i].fill_between(range(0,96), l, lmean, color=markers[i], alpha=0.1)\n",
    "        #ax[i].set_xlabel()\n",
    "        \n",
    "plt.gcf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tf.keras.models.save_model(model, '_TFmodel')\n",
    "tf.keras.models.save_model(symencoder, '_TFencoderr')\n",
    "tf.keras.models.save_model(nencoder, '_TFencodertau')\n",
    "# tf.keras.models.save_model(encoder, '_TFencoder')\n",
    "tf.keras.models.save_model(decoder, '_TFdecoder')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
